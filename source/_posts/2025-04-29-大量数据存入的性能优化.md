---
title: 大量数据批量存入的性能优化
date: 2025-04-29 11:05:13
categories: # 分类
	- 技术点
tags: 
    - 大量数据插入
---

当在数据量比较大需要插入时，进行逐条插入MySQL可能会速度很慢，所以这时有几种思路来进行优化

## 1.使用mybatisPlus的saveBatch方法进行批量插入

**service调用**

 ```Java
 @Service
 @Slf4j
 public class BookServiceImpl extends ServiceImpl<BookMapper, Book>
     implements BookService {
 
     public void concurrentBatchInsert(List<Book> books, int batchSize) {
 
         // 数据分片，需要引入hutool依赖，给books中的100w数据进行分片，每个list子集合1000条数据
         List<List<Book>> batches = ListUtil.partition(books, batchSize);
         long startTime = System.currentTimeMillis();
 
         for (List<Book> batch : batches) {
             log.info("开始批量插入数据");
             this.saveBatch(batch);
         }
         long endTime = System.currentTimeMillis();
         log.info("耗费时间："+String.valueOf(endTime-startTime)+"ms");
     }
 
 }
 ```



**插入100w数据耗时**

```
耗费时间：34723ms
```

## 2.使用foreach进行sql拼接，来进行批量插入

**service调用逻辑**

 ```Java
 @Service
 @Slf4j
 public class BookServiceImpl extends ServiceImpl<BookMapper, Book>
     implements BookService {
 
     @Autowired
     private BookMapper bookMapper;
 
     public void concurrentBatchInsert(List<Book> books, int batchSize) {
 
         // 数据分片，需要引入hutool依赖，给books中的10w数据进行分片，每个list子集合1000条数据
         List<List<Book>> batches = ListUtil.partition(books, batchSize);
         long startTime = System.currentTimeMillis();
 
         for (List<Book> batch : batches) {
             log.info("开始批量插入数据");
             bookMapper.insertBatchSomeColumn(batch);
         }
         long endTime = System.currentTimeMillis();
         log.info("耗费时间："+String.valueOf(endTime-startTime)+"ms");
     }
 
 }
 ```



**mapper接口层**

```Java
public interface BookMapper extends BaseMapper<Book> {
    void insertBatchSomeColumn(@Param("batch") List<Book> batch);
}
```

**mapper.xml**

```Java
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.sky.work_test.mapper.BookMapper">

    <insert id="insertBatchSomeColumn">
        INSERT INTO book (bkname,sid,price,author,sur)
        VALUES
        <foreach collection="batch" item="item" separator=",">
            (#{item.bkname}, #{item.sid},#{item.price},#{item.author},#{item.sur})
        </foreach>
    </insert>
</mapper>

```



**插入100w数据耗时**

```
耗费时间：29760ms
```



 

## 3.在数据量很大的时候可以考虑使用线程池来异步并发的执行插入任务

**此处使用线程池异步并发来处理批量任务插入，插入方法可以是sql拼接或者mybatisPlus的saveBatch**

 ```Java
 @Service
 @Slf4j
 public class BookServiceImpl extends ServiceImpl<BookMapper, Book>
     implements BookService {
 
     @Autowired
     private BookMapper bookMapper;
 
     @Resource(name = "dataProcessingExecutor")
     private ThreadPoolTaskExecutor  dataProcessingExecutor;
 
     @Async("dataProcessingExecutor")//异步执行
     @Transactional(rollbackFor = Exception.class,propagation = Propagation.REQUIRES_NEW)
     public void concurrentBatchInsert(List<Book> books, int batchSize) {
         //创建list集合接受线程的返回值
         List<CompletableFuture<Void>> futures = new ArrayList<>();
 
         // 数据分片，需要引入hutool依赖，给books中的100w数据进行分片，每个list子集合存放1000条数据
         List<List<Book>> batches = ListUtil.partition(books, batchSize);
         long startTime = System.currentTimeMillis();
 
         for (List<Book> batch : batches) {
             log.info("开始批量插入数据");
             //开启多个线程并行执行
             CompletableFuture<Void> future = CompletableFuture.runAsync(() -> {
                 bookMapper.insertBatchSomeColumn(batch);
             }, dataProcessingExecutor);
 
             futures.add(future);
         }
 
         // 等待所有数据插入完成后退出阻塞
         CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])).join();
         long endTime = System.currentTimeMillis();
         log.info(String.valueOf(endTime-startTime));
     }
 
 }
 ```



**使用foreach的sql拼接插入100w数据耗时**

```
耗费时间：7024ms
```



**使用mybatisPlus的saveBatch方法进行插入100w数据耗时**

```
耗费时间：7565ms
```





## 测试插入

```Java
@RestController
@Slf4j
public class BookController {

    @Autowired
    private BookMapper bookMapper;
    @Autowired
    private BookServiceImpl bookService;
    @GetMapping("/insert")
    public void insertLargeData() {
        List<Book> data = new ArrayList<>();
        for (int i = 0; i < 1000000; i++) {
            //主键设置为自增
            Book book = new Book();
            book.setSid("s99");
            book.setBkname("sky");
            book.setPrice(99);
            book.setAuthor("sky");
            book.setSur(99);
            data.add(book);
        }
        bookService.concurrentBatchInsert(data,1000);
    }

}
```

